# Conclusiones

En el presente trabajo se ha demostrado la utilidad de **PySpark** como una herramienta de manejo y transformación de datos y la utilidad de la librería **MRJob** como una herramienta de implementación de algoritmos de machine learning. 

Con respecto a **PySpark**, se ha podido validar que es una herramienta capaz de optimizar cálculos en tablas gracias a sus objetos del tipo DataFrame. La característica de _lazy evaluation_ de la librería le permite encontrar los pasos más eficientes para transformar una tabla al estado que se requiere. La posibilidad de almacenar una tabla en memoria _cache_ también acelera la velocidad de procesamiento.

En relación a la herramienta **MRJob**, se puede concluir que esta engloba una serie de clases y métodos que pueden ser fácilmente redefinidos para implementar un algoritmo de machine learning bajo el paradigma de _map reduce_ de forma práctica y legible. Cabe resaltar que se hizo latente un desafío al utilizar esta herramienta. Debido a que el método _mapper_ se repite por cada registro, imposibilita el guardado de datos obtenidos en esta etapa. Para solucionarlo, se debe recurrir a la definición de métodos externos para proceder con el almacenamiento de resultados, esto genera así un costo computacional.

Algunos **oportunidades de mejora** que ha identificado el equipo son las siguientes. Actualmente el _Silhouette Score_ alcanzado por los clusters es de 0.14. Este número podría aumentar hasta llegar a un valor cercano a 1, por lo tanto, se pueden calibrar parámetros de los algoritmos que interfieren (KMeans, PCA, Fuzzy C-Means) para obtener un mejor score. Por otro lado, con un poco más de tiempo de desarrollo disponible, habría sido muy provechoso implementar el algoritmo Fuzzy C-Means en PySpark y poder comparar la rapidez de computación y la calidad de los clusters resultantes. Finalmente, la obtención de mayor cantidad de datos siempre es una opción para mejorar la calidad del output.
